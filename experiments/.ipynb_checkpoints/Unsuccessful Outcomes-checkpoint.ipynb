{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from epilepsypcm.utils.outcome_params import seizure_onset_zone, engel_score, early_propogation, irritative_zone\n",
    "\n",
    "# INPUT\n",
    "# patient = string format, patient number\n",
    "# paths = path to CCEP response files, in os format\n",
    "# OUTPUT\n",
    "# df = dataframe for one patient with\n",
    "#       X features with columns: chNames, significant, n1, n2, p2 z scores,\n",
    "#       n1, n2, p2 latencies, and flipped\n",
    "#       and associated y outcome labels\n",
    "def make_df(patient, paths):\n",
    "    #extracting info from each response file\n",
    "    n = 0\n",
    "    stimChs = []\n",
    "    for i in range(len(paths)):\n",
    "        chNames = []\n",
    "        # load info into python dictionary\n",
    "        data = json.load(open(paths[i]))\n",
    "        p_name = patient\n",
    "\n",
    "        # Get list of channel names\n",
    "        for key in data[\"time\"]: chNames.append(key)\n",
    "        # loop over each channel, and extract average time series and information about the peaks\n",
    "\n",
    "        if n < 1:\n",
    "            avgResp = np.empty((len(paths), len(chNames), len(data['time'][chNames[0]])))\n",
    "            significant = np.empty((len(paths), len(chNames)))\n",
    "            n1Zscore = np.empty((len(paths), len(chNames)))\n",
    "            n2Zscore = np.empty((len(paths), len(chNames)))\n",
    "            p2Zscore = np.empty((len(paths), len(chNames)))\n",
    "            n1Latency = np.empty((len(paths), len(chNames)))\n",
    "            n2Latency = np.empty((len(paths), len(chNames)))\n",
    "            p2Latency = np.empty((len(paths), len(chNames)))\n",
    "            flipped = np.empty((len(paths), len(chNames)))\n",
    "            n += 1\n",
    "            samplingRate = np.empty((len(paths)))\n",
    "            window = np.empty((len(paths), 2))\n",
    "\n",
    "        for j in range(len(chNames)):\n",
    "            avgResp[i][j] = data['time'][chNames[j]]\n",
    "            significant[i][j] = data['significant'][chNames[j]]\n",
    "            n1Zscore[i][j] = data['zscores'][chNames[j]]['n1'][1]\n",
    "            n2Zscore[i][j] = data['zscores'][chNames[j]]['n2'][1]\n",
    "            p2Zscore[i][j] = data['zscores'][chNames[j]]['p2'][1]\n",
    "            n1Latency[i][j] = data['zscores'][chNames[j]]['n1'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            n2Latency[i][j] = data['zscores'][chNames[j]]['n2'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            p2Latency[i][j] = data['zscores'][chNames[j]]['p2'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            flipped[i][j] = data['zscores'][chNames[j]]['flipped']\n",
    "\n",
    "        samplingRate[i] = data[\"samplingRate\"]\n",
    "        window[i] = data['window']\n",
    "        stimChs = stimChs + [paths[i].split(\"_\")[1] + \"_\" + paths[i].split(\"_\")[2]]*len(chNames)\n",
    "\n",
    "\n",
    "    # creating dataframe\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"stimChs\"] = stimChs\n",
    "    df[\"respChs\"] = chNames * len(paths)\n",
    "    df[\"significant\"] = significant.flatten()\n",
    "    df[\"n1Zscore\"] = n1Zscore.flatten()\n",
    "    df[\"n2Zscore\"] = n2Zscore.flatten()\n",
    "    df[\"p2Zscore\"] = p2Zscore.flatten()\n",
    "    df[\"n1Latency\"] = n1Latency.flatten()\n",
    "    df[\"n2Latency\"] = n2Latency.flatten()\n",
    "    df[\"p2Latency\"] = p2Latency.flatten()\n",
    "    df[\"flipped\"] = flipped.flatten()\n",
    "    df[\"patient\"] = p_name\n",
    "\n",
    "    # Dropped rows for stimulating channels since they only\n",
    "    # contain stimulating waveforms / artifacts / saturated signals\n",
    "    # Also zero out rows with latency values of -999.0\n",
    "\n",
    "    # drop rows in the dataframe with latency values of -999.0\n",
    "    df = df.drop(df.loc[df[\"n1Latency\"] == -999.0].index)\n",
    "    df = df.drop(df.loc[df[\"n1Latency\"] == -499.0].index)\n",
    "\n",
    "    # adding dataframe outcome values (1 if in SOZ, 0 if not)\n",
    "\n",
    "    \n",
    "    # adding dataframe outcome values (1 if in SOZ, 0 if not)\n",
    "    df[\"outcome\"] = np.zeros(df.shape[0])\n",
    "    df[\"IZ\"] = np.zeros(df.shape[0])\n",
    "    df[\"EP\"] = np.zeros(df.shape[0])\n",
    "    \n",
    "    if engel_score[patient] == \"3\":\n",
    "        if seizure_onset_zone[patient] != [\"None\"]:\n",
    "            for node in seizure_onset_zone[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    channel_split = channel.split(\"_\")\n",
    "                    if (node == channel_split[0]) | (node == channel_split[1]):\n",
    "                        df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "                    elif (\"0\" in channel_split[0][1:-1]) | (\"0\" in channel_split[1][1:-1]): # LA01 vs LA1\n",
    "                        if \"0\" in channel_split[0][1:-1]:\n",
    "                            channel_new = channel_split[0].replace(\"0\", \"\")\n",
    "                            if node == channel_new:\n",
    "                                df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "                        if \"0\" in channel_split[1][1:-1]:\n",
    "                            channel_new = channel_split[1].replace(\"0\", \"\")\n",
    "                            if node == channel_new:\n",
    "                                df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "\n",
    "        if irritative_zone[patient] != [\"None\"]:\n",
    "            for IZnode in irritative_zone[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    channel_split = channel.split(\"_\")\n",
    "                    if (IZnode == channel_split[0]) | (IZnode == channel_split[1]):\n",
    "                        df.loc[df['respChs']==channel, ['IZ']] = 1\n",
    "        if early_propogation[patient] != [\"None\"]:\n",
    "            for EPnode in early_propogation[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    channel_split = channel.split(\"_\")\n",
    "                    if (EPnode == channel_split[0]) | (EPnode == channel_split[1]):\n",
    "                        df.loc[df['respChs']==channel, ['EP']] = 1\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_processing(D):\n",
    "    D.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    #Find channel names that exists both in stimChs and respChs - only account channels that have arrows going out and in\n",
    "    overlap = []\n",
    "    for channel in D.respChs.unique():\n",
    "        if channel in D.stimChs.unique():\n",
    "            overlap.append(channel)\n",
    "\n",
    "    #Keep only the response that were stimulated in responded in the channel in overlap list\n",
    "    dropindxs = []\n",
    "    for i in range(len(D)):\n",
    "        if D.iloc[i].stimChs not in overlap or D.iloc[i].respChs not in overlap:\n",
    "                dropindxs.append(i)\n",
    "    D.drop(dropindxs,inplace=True)\n",
    "    D.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    D.n1Zscore = abs(D.n1Zscore)\n",
    "    D.n2Zscore = abs(D.n2Zscore)\n",
    "    D.p2Zscore = abs(D.p2Zscore)\n",
    "    \n",
    "    #start processing\n",
    "    df = pd.DataFrame()\n",
    "    ChNames = overlap\n",
    "    Outcomes = np.array([])\n",
    "    IZ = np.array([])\n",
    "    EP = np.array([])\n",
    "    Per_Significant_Resp = np.array([])\n",
    "    Per_Significant_Stim = np.array([])\n",
    "    N1_Avg_Resp = np.array([])\n",
    "    N1_STV_Resp = np.array([])\n",
    "    N2_Avg_Resp = np.array([])\n",
    "    N2_STV_Resp = np.array([])\n",
    "    P2_Avg_Resp = np.array([])\n",
    "    P2_STV_Resp = np.array([])\n",
    "    N1_Avg_Stim = np.array([])\n",
    "    N1_STV_Stim = np.array([])\n",
    "    N2_Avg_Stim = np.array([])\n",
    "    N2_STV_Stim = np.array([])\n",
    "    P2_Avg_Stim = np.array([])\n",
    "    P2_STV_Stim = np.array([])\n",
    "\n",
    "    for channel in ChNames:\n",
    "        Resp = D[D.respChs == channel]\n",
    "        Stim = D[D.stimChs == channel]\n",
    "\n",
    "        Outcomes = np.append(Outcomes,Resp[:1].outcome)\n",
    "        IZ = np.append(IZ,Resp[:1].IZ)\n",
    "        EP = np.append(EP,Resp[:1].EP)\n",
    "\n",
    "        Per_Significant_Resp = np.append(Per_Significant_Resp,\n",
    "                                         sum(Resp.significant/len(Resp)))\n",
    "        Per_Significant_Stim = np.append(Per_Significant_Stim,\n",
    "                                         sum(Stim.significant/len(Stim)))\n",
    "\n",
    "        N1_Avg_Resp = np.append(N1_Avg_Resp,sum(Resp.n1Zscore)/len(Resp))\n",
    "        N1_STV_Resp = np.append(N1_STV_Resp,np.std(Resp.n1Zscore))\n",
    "\n",
    "        N2_Avg_Resp = np.append(N2_Avg_Resp,sum(Resp.n2Zscore)/len(Resp))\n",
    "        N2_STV_Resp = np.append(N2_STV_Resp,np.std(Resp.n2Zscore))\n",
    "\n",
    "        P2_Avg_Resp = np.append(P2_Avg_Resp,sum(Resp.p2Zscore)/len(Resp))\n",
    "        P2_STV_Resp = np.append(P2_STV_Resp,np.std(Resp.p2Zscore))\n",
    "\n",
    "        N1_Avg_Stim = np.append(N1_Avg_Stim,sum(Stim.n1Zscore)/len(Stim))\n",
    "        N1_STV_Stim = np.append(N1_STV_Stim,np.std(Stim.n1Zscore))\n",
    "\n",
    "        N2_Avg_Stim = np.append(N2_Avg_Stim,sum(Stim.n2Zscore)/len(Stim))\n",
    "        N2_STV_Stim = np.append(N2_STV_Stim,np.std(Stim.n2Zscore))\n",
    "\n",
    "        P2_Avg_Stim = np.append(P2_Avg_Stim,sum(Stim.p2Zscore)/len(Stim))\n",
    "        P2_STV_Stim = np.append(P2_STV_Stim,np.std(Stim.p2Zscore))\n",
    "\n",
    "\n",
    "    df['Channels'] = ChNames\n",
    "    df['outcome'] = Outcomes\n",
    "    df['IZ'] = IZ\n",
    "    df['EP'] = EP\n",
    "    df['SigResp'] = Per_Significant_Resp\n",
    "    df['SigStim'] = Per_Significant_Stim\n",
    "    df['N1RespAvg'] = N1_Avg_Resp\n",
    "    df['N1RespSDV'] = N1_STV_Resp\n",
    "    df['N2RespAvg'] = N2_Avg_Resp\n",
    "    df['N2RespSDV'] = N2_STV_Resp\n",
    "    df['P2RespAvg'] = P2_Avg_Resp\n",
    "    df['P2RespSDV'] = P2_STV_Resp\n",
    "    df['N1StimAvg'] = N1_Avg_Stim\n",
    "    df['N1StimSDV'] = N1_STV_Stim\n",
    "    df['N2StimAvg'] = N2_Avg_Stim\n",
    "    df['N2StimSDV'] = N2_STV_Stim\n",
    "    df['P2StimAvg'] = P2_Avg_Stim\n",
    "    df['P2StimSDV'] = P2_STV_Stim\n",
    "    df['patient'] = D.iloc[0].patient\n",
    "    \n",
    "    df[\"InDegree\"] = np.zeros(df.shape[0])\n",
    "    df[\"OutDegree\"] = np.zeros(df.shape[0])\n",
    "    df[\"EV\"] = np.zeros(df.shape[0])\n",
    "    df[\"Closeness\"] = np.zeros(df.shape[0])\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    for i in range(D.shape[0]):\n",
    "        if D.significant.iloc[i] == 1:\n",
    "            G.add_edge(D.stimChs.iloc[i],D.respChs.iloc[i])\n",
    "\n",
    "    EV_Centrality = nx.eigenvector_centrality(G)\n",
    "    Closeness_Centrality = nx.closeness_centrality(G)\n",
    "    InDegree = nx.in_degree_centrality(G)\n",
    "    OutDegree = nx.out_degree_centrality(G)\n",
    "    \n",
    "    for channel in list(EV_Centrality):\n",
    "        df.loc[df.Channels == channel, 'EV'] = EV_Centrality[channel]\n",
    "    for channel in list(Closeness_Centrality):\n",
    "        df.loc[df.Channels == channel, 'Closeness'] = Closeness_Centrality[channel]\n",
    "    for channel in list(InDegree):\n",
    "        df.loc[df.Channels == channel, 'InDegree'] = InDegree[channel]\n",
    "    for channel in list(OutDegree):\n",
    "        df.loc[df.Channels == channel, 'OutDegree'] = OutDegree[channel]\n",
    "\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_df_list(base_path, engel):\n",
    "    patient_files = os.listdir(base_path)\n",
    "\n",
    "    positive_dataframes = []\n",
    "    for file in patient_files:\n",
    "        if (file[0] == \"P\") & (file != \"PY16N006\"):\n",
    "            response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "            response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "\n",
    "            # Getting individual dataframe for positive patients\n",
    "            patient = file\n",
    "            if file in engel_score.keys():  # if we currently have the file's engel score\n",
    "                if engel_score[patient] == engel:  # if the engel score is 1\n",
    "                    df = make_df(patient, response_files_path)\n",
    "                    positive_dataframes.append([patient, df])\n",
    "\n",
    "    return positive_dataframes\n",
    "\n",
    "# Function that combines dataframes for all patients of a particular\n",
    "# engel class\n",
    "# INPUT:\n",
    "# base_path = string, file location to base folder that contains all patient folders\n",
    "# engel_score = string, target engel score to get dataframe for (ex. \"1\")\n",
    "#               can currently only handle \"1\" and \"2\"\n",
    "# balance (OPTIONAL, default = None) = \"None\", \"upsample\", or \"downsample\"\n",
    "#          will upsample minority class or downsample majority class to balance\n",
    "#           the data\n",
    "# OUTPUT:\n",
    "# all_positive_patients = a concatonated dataframe of all patients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def concat_dfs(base_path, engel, balance = None):\n",
    "    \n",
    "    patient_files = os.listdir(base_path)\n",
    "\n",
    "    full_df = pd.DataFrame()\n",
    "    for file in patient_files:\n",
    "        if (file[0] == \"P\") & (file != \"PY16N006\") & (file != 'PY17N014'): #PY17N014 was eliminated because there is no node with significant response\n",
    "            response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "            response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "\n",
    "            # Getting individual dataframe for positive patients\n",
    "            patient = file\n",
    "            if file in engel_score.keys():  # if we currently have the file's engel score\n",
    "                if engel_score[patient] == engel:  # if the engel score is 1\n",
    "                    df = make_df(patient, response_files_path)\n",
    "                    df = df_processing(df)\n",
    "                    full_df = pd.concat([full_df, df])\n",
    "                    \n",
    "                    print('%s done...'%patient)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
