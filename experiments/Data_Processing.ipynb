{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'epilepsypcm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_d/sn_gw90x46b72cj5db5_bfx40000gn/T/ipykernel_7379/3843023692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mepilepsypcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome_params\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseizure_onset_zone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengel_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_propogation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirritative_zone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mepilepsypcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'epilepsypcm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from epilepsypcm.utils.outcome_params import seizure_onset_zone, engel_score, early_propogation, irritative_zone\n",
    "from sklearn.metrics import auc\n",
    "from epilepsypcm.models.base_models import *\n",
    "\n",
    "# INPUT\n",
    "# patient = string format, patient number\n",
    "# paths = path to CCEP response files, in os format\n",
    "# OUTPUT\n",
    "# df = dataframe for one patient with\n",
    "#       X features with columns: chNames, significant, n1, n2, p2 z scores,\n",
    "#       n1, n2, p2 latencies, and flipped\n",
    "#       and associated y outcome labels\n",
    "def make_df(patient, paths):\n",
    "    #extracting info from each response file\n",
    "    n = 0\n",
    "    stimChs = []\n",
    "    for i in range(len(paths)):\n",
    "        chNames = []\n",
    "        # load info into python dictionary\n",
    "        data = json.load(open(paths[i]))\n",
    "        p_name = patient\n",
    "\n",
    "        # Get list of channel names\n",
    "        for key in data[\"time\"]: chNames.append(key)\n",
    "        # loop over each channel, and extract average time series and information about the peaks\n",
    "\n",
    "        if n < 1:\n",
    "            avgResp = np.empty((len(paths), len(chNames), len(data['time'][chNames[0]])))\n",
    "            significant = np.empty((len(paths), len(chNames)))\n",
    "            n1Zscore = np.empty((len(paths), len(chNames)))\n",
    "            n2Zscore = np.empty((len(paths), len(chNames)))\n",
    "            p2Zscore = np.empty((len(paths), len(chNames)))\n",
    "            n1Latency = np.empty((len(paths), len(chNames)))\n",
    "            n2Latency = np.empty((len(paths), len(chNames)))\n",
    "            p2Latency = np.empty((len(paths), len(chNames)))\n",
    "            flipped = np.empty((len(paths), len(chNames)))\n",
    "            n += 1\n",
    "            samplingRate = np.empty((len(paths)))\n",
    "            window = np.empty((len(paths), 2))\n",
    "\n",
    "        for j in range(len(chNames)):\n",
    "            avgResp[i][j] = data['time'][chNames[j]]\n",
    "            significant[i][j] = data['significant'][chNames[j]]\n",
    "            n1Zscore[i][j] = data['zscores'][chNames[j]]['n1'][1]\n",
    "            n2Zscore[i][j] = data['zscores'][chNames[j]]['n2'][1]\n",
    "            p2Zscore[i][j] = data['zscores'][chNames[j]]['p2'][1]\n",
    "            n1Latency[i][j] = data['zscores'][chNames[j]]['n1'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            n2Latency[i][j] = data['zscores'][chNames[j]]['n2'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            p2Latency[i][j] = data['zscores'][chNames[j]]['p2'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            flipped[i][j] = data['zscores'][chNames[j]]['flipped']\n",
    "        samplingRate[i] = data[\"samplingRate\"]\n",
    "        window[i] = data['window']\n",
    "        stimChs = stimChs + [paths[i].split(\"_\")[1] + \"_\" + paths[i].split(\"_\")[2]]*len(chNames)\n",
    "\n",
    "\n",
    "    # creating dataframe\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"stimChs\"] = stimChs\n",
    "    df[\"respChs\"] = chNames * len(paths)\n",
    "    df[\"significant\"] = significant.flatten()\n",
    "    df[\"n1Zscore\"] = n1Zscore.flatten()\n",
    "    df[\"n2Zscore\"] = n2Zscore.flatten()\n",
    "    df[\"p2Zscore\"] = p2Zscore.flatten()\n",
    "    df[\"n1Latency\"] = n1Latency.flatten()\n",
    "    df[\"n2Latency\"] = n2Latency.flatten()\n",
    "    df[\"p2Latency\"] = p2Latency.flatten()\n",
    "    df[\"flipped\"] = flipped.flatten()\n",
    "    df[\"patient\"] = p_name\n",
    "\n",
    "    # Dropped rows for stimulating channels since they only\n",
    "    # contain stimulating waveforms / artifacts / saturated signals\n",
    "    # Also zero out rows with latency values of -999.0\n",
    "\n",
    "    # drop rows in the dataframe with latency values of -999.0\n",
    "    df = df.drop(df.loc[df[\"n1Latency\"] == -999.0].index)\n",
    "    df = df.drop(df.loc[df[\"n1Latency\"] == -499.0].index)\n",
    "\n",
    "    # adding dataframe outcome values (1 if in SOZ, 0 if not)\n",
    "\n",
    "    \n",
    "    # adding dataframe outcome values (1 if in SOZ, 0 if not)\n",
    "    df[\"outcome\"] = np.zeros(df.shape[0])\n",
    "    df[\"IZ\"] = np.zeros(df.shape[0])\n",
    "    df[\"EP\"] = np.zeros(df.shape[0])\n",
    "    \n",
    "    if engel_score[patient] == \"1\":\n",
    "        if seizure_onset_zone[patient] != [\"None\"]:\n",
    "            for node in seizure_onset_zone[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    channel_split = channel.split(\"_\")\n",
    "                    if (node == channel_split[0]) | (node == channel_split[1]):\n",
    "                        df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "                    elif (\"0\" in channel_split[0][1:-1]) | (\"0\" in channel_split[1][1:-1]): # LA01 vs LA1\n",
    "                        if \"0\" in channel_split[0][1:-1]:\n",
    "                            channel_new = channel_split[0].replace(\"0\", \"\")\n",
    "                            if node == channel_new:\n",
    "                                df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "                        if \"0\" in channel_split[1][1:-1]:\n",
    "                            channel_new = channel_split[1].replace(\"0\", \"\")\n",
    "                            if node == channel_new:\n",
    "                                df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "\n",
    "        if irritative_zone[patient] != [\"None\"]:\n",
    "            for IZnode in irritative_zone[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    channel_split = channel.split(\"_\")\n",
    "                    if (IZnode == channel_split[0]) | (IZnode == channel_split[1]):\n",
    "                        df.loc[df['respChs']==channel, ['IZ']] = 1\n",
    "        if early_propogation[patient] != [\"None\"]:\n",
    "            for EPnode in early_propogation[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    channel_split = channel.split(\"_\")\n",
    "                    if (EPnode == channel_split[0]) | (EPnode == channel_split[1]):\n",
    "                        df.loc[df['respChs']==channel, ['EP']] = 1\n",
    "                \n",
    "    return df\n",
    "\n",
    "# Function that takes in the location of all patient folders and engel\n",
    "# score of interest, and returns a nested list of dataframes for each patient\n",
    "# INPUT:\n",
    "# base_path = string, file location to base folder that contains all patient folders\n",
    "# engel_score = string, target engel score to get dataframe for (ex. \"1\")\n",
    "#               can currently only handle \"1\" and \"2\"\n",
    "# OUTPUT:\n",
    "# positive_dataframes = a nested list, where [patient number (string), dataframe].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_processing(D):\n",
    "    D.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    #Find channel names that exists both in stimChs and respChs - only account channels that have arrows going out and in\n",
    "    overlap = []\n",
    "    for channel in D.respChs.unique():\n",
    "        if channel in D.stimChs.unique():\n",
    "            overlap.append(channel)\n",
    "\n",
    "    #Keep only the response that were stimulated in responded in the channel in overlap list\n",
    "    dropindxs = []\n",
    "    for i in range(len(D)):\n",
    "        if D.iloc[i].stimChs not in overlap or D.iloc[i].respChs not in overlap:\n",
    "                dropindxs.append(i)\n",
    "    D.drop(dropindxs,inplace=True)\n",
    "    D.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    D.n1Zscore = abs(D.n1Zscore)\n",
    "    D.n2Zscore = abs(D.n2Zscore)\n",
    "    D.p2Zscore = abs(D.p2Zscore)\n",
    "    \n",
    "    #start processing\n",
    "    df = pd.DataFrame()\n",
    "    ChNames = overlap\n",
    "    Outcomes = np.array([])\n",
    "    IZ = np.array([])\n",
    "    EP = np.array([])\n",
    "    Per_Significant_Resp = np.array([])\n",
    "    Per_Significant_Stim = np.array([])\n",
    "    N1_Avg_Resp = np.array([])\n",
    "    N1_STV_Resp = np.array([])\n",
    "    N2_Avg_Resp = np.array([])\n",
    "    N2_STV_Resp = np.array([])\n",
    "    P2_Avg_Resp = np.array([])\n",
    "    P2_STV_Resp = np.array([])\n",
    "    N1_Avg_Stim = np.array([])\n",
    "    N1_STV_Stim = np.array([])\n",
    "    N2_Avg_Stim = np.array([])\n",
    "    N2_STV_Stim = np.array([])\n",
    "    P2_Avg_Stim = np.array([])\n",
    "    P2_STV_Stim = np.array([])\n",
    "\n",
    "    for channel in ChNames:\n",
    "        Resp = D[D.respChs == channel]\n",
    "        Stim = D[D.stimChs == channel]\n",
    "\n",
    "        Outcomes = np.append(Outcomes,Resp[:1].outcome)\n",
    "        IZ = np.append(IZ,Resp[:1].IZ)\n",
    "        EP = np.append(EP,Resp[:1].EP)\n",
    "\n",
    "        Per_Significant_Resp = np.append(Per_Significant_Resp,\n",
    "                                         sum(Resp.significant/len(Resp)))\n",
    "        Per_Significant_Stim = np.append(Per_Significant_Stim,\n",
    "                                         sum(Stim.significant/len(Stim)))\n",
    "\n",
    "        N1_Avg_Resp = np.append(N1_Avg_Resp,sum(Resp.n1Zscore)/len(Resp))\n",
    "        N1_STV_Resp = np.append(N1_STV_Resp,np.std(Resp.n1Zscore))\n",
    "\n",
    "        N2_Avg_Resp = np.append(N2_Avg_Resp,sum(Resp.n2Zscore)/len(Resp))\n",
    "        N2_STV_Resp = np.append(N2_STV_Resp,np.std(Resp.n2Zscore))\n",
    "\n",
    "        P2_Avg_Resp = np.append(P2_Avg_Resp,sum(Resp.p2Zscore)/len(Resp))\n",
    "        P2_STV_Resp = np.append(P2_STV_Resp,np.std(Resp.p2Zscore))\n",
    "\n",
    "        N1_Avg_Stim = np.append(N1_Avg_Stim,sum(Stim.n1Zscore)/len(Stim))\n",
    "        N1_STV_Stim = np.append(N1_STV_Stim,np.std(Stim.n1Zscore))\n",
    "\n",
    "        N2_Avg_Stim = np.append(N2_Avg_Stim,sum(Stim.n2Zscore)/len(Stim))\n",
    "        N2_STV_Stim = np.append(N2_STV_Stim,np.std(Stim.n2Zscore))\n",
    "\n",
    "        P2_Avg_Stim = np.append(P2_Avg_Stim,sum(Stim.p2Zscore)/len(Stim))\n",
    "        P2_STV_Stim = np.append(P2_STV_Stim,np.std(Stim.p2Zscore))\n",
    "\n",
    "\n",
    "    df['Channels'] = ChNames\n",
    "    df['outcome'] = Outcomes\n",
    "    df['IZ'] = IZ\n",
    "    df['EP'] = EP\n",
    "    df['SigResp'] = Per_Significant_Resp\n",
    "    df['SigStim'] = Per_Significant_Stim\n",
    "    df['N1RespAvg'] = N1_Avg_Resp\n",
    "    df['N1RespSDV'] = N1_STV_Resp\n",
    "    df['N2RespAvg'] = N2_Avg_Resp\n",
    "    df['N2RespSDV'] = N2_STV_Resp\n",
    "    df['P2RespAvg'] = P2_Avg_Resp\n",
    "    df['P2RespSDV'] = P2_STV_Resp\n",
    "    df['N1StimAvg'] = N1_Avg_Stim\n",
    "    df['N1StimSDV'] = N1_STV_Stim\n",
    "    df['N2StimAvg'] = N2_Avg_Stim\n",
    "    df['N2StimSDV'] = N2_STV_Stim\n",
    "    df['P2StimAvg'] = P2_Avg_Stim\n",
    "    df['P2StimSDV'] = P2_STV_Stim\n",
    "    df['patient'] = D.iloc[0].patient\n",
    "    \n",
    "    df[\"InDegree\"] = np.zeros(df.shape[0])\n",
    "    df[\"OutDegree\"] = np.zeros(df.shape[0])\n",
    "    df[\"EV\"] = np.zeros(df.shape[0])\n",
    "    df[\"Closeness\"] = np.zeros(df.shape[0])\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    for i in range(D.shape[0]):\n",
    "        if D.significant.iloc[i] == 1:\n",
    "            G.add_edge(D.stimChs.iloc[i],D.respChs.iloc[i])\n",
    "\n",
    "    EV_Centrality = nx.eigenvector_centrality(G)\n",
    "    Closeness_Centrality = nx.closeness_centrality(G)\n",
    "    InDegree = nx.in_degree_centrality(G)\n",
    "    OutDegree = nx.out_degree_centrality(G)\n",
    "    \n",
    "    for channel in list(EV_Centrality):\n",
    "        df.loc[df.Channels == channel, 'EV'] = EV_Centrality[channel]\n",
    "    for channel in list(Closeness_Centrality):\n",
    "        df.loc[df.Channels == channel, 'Closeness'] = Closeness_Centrality[channel]\n",
    "    for channel in list(InDegree):\n",
    "        df.loc[df.Channels == channel, 'InDegree'] = InDegree[channel]\n",
    "    for channel in list(OutDegree):\n",
    "        df.loc[df.Channels == channel, 'OutDegree'] = OutDegree[channel]\n",
    "\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_df_list(base_path, engel):\n",
    "    patient_files = os.listdir(base_path)\n",
    "\n",
    "    positive_dataframes = []\n",
    "    for file in patient_files:\n",
    "        if (file[0] == \"P\") & (file != \"PY16N006\"):\n",
    "            response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "            response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "\n",
    "            # Getting individual dataframe for positive patients\n",
    "            patient = file\n",
    "            if file in engel_score.keys():  # if we currently have the file's engel score\n",
    "                if engel_score[patient] == engel:  # if the engel score is 1\n",
    "                    df = make_df(patient, response_files_path)\n",
    "                    positive_dataframes.append([patient, df])\n",
    "\n",
    "    return positive_dataframes\n",
    "\n",
    "# Function that combines dataframes for all patients of a particular\n",
    "# engel class\n",
    "# INPUT:\n",
    "# base_path = string, file location to base folder that contains all patient folders\n",
    "# engel_score = string, target engel score to get dataframe for (ex. \"1\")\n",
    "#               can currently only handle \"1\" and \"2\"\n",
    "# balance (OPTIONAL, default = None) = \"None\", \"upsample\", or \"downsample\"\n",
    "#          will upsample minority class or downsample majority class to balance\n",
    "#           the data\n",
    "# OUTPUT:\n",
    "# all_positive_patients = a concatonated dataframe of all patients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def concat_dfs(base_path, engel, balance = None):\n",
    "    \n",
    "    patient_files = os.listdir(base_path)\n",
    "\n",
    "    full_df = pd.DataFrame()\n",
    "    for file in patient_files:\n",
    "        if (file[0] == \"P\") & (file != \"PY16N006\") & (file != 'PY17N014'): #PY17N014 was eliminated because there is no node with significant response\n",
    "            response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "            response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "\n",
    "            # Getting individual dataframe for positive patients\n",
    "            patient = file\n",
    "            if file in engel_score.keys():  # if we currently have the file's engel score\n",
    "                if engel_score[patient] == engel:  # if the engel score is 1\n",
    "                    df = make_df(patient, response_files_path)\n",
    "                    df = df_processing(df)\n",
    "                    full_df = pd.concat([full_df, df])\n",
    "                    \n",
    "                    print('%s done...'%patient)\n",
    "\n",
    "    # seperate dataframes for class\n",
    "    df_majority = full_df[full_df.outcome == 0]\n",
    "    df_minority = full_df[full_df.outcome == 1]\n",
    "\n",
    "    # upsample data if balance parameter is set to \"Upsample\" or \"upsample\"\n",
    "    if (balance == \"upsample\") | (balance == \"Upsample\"):\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                         replace=True,  # sample with replacement\n",
    "                                         n_samples=full_df[\"outcome\"].value_counts()[0.0],\n",
    "                                         # to match majority class\n",
    "                                         random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        # combine dataframes\n",
    "        full_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # downsample data if balance parameter is set to \"downsample\" or \"Downsample\"\n",
    "    elif (balance == \"downsample\") | (balance == \"Downsample\"):\n",
    "        # downsample majority class\n",
    "        # downsample majority class\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                           replace=False,  # sample without replacement\n",
    "                                           n_samples= full_df[\"outcome\"].value_counts()[1.0],\n",
    "                                           # to match minority class\n",
    "                                           random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        full_df = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    return full_df\n",
    "\n",
    "\n",
    "# Function that upsamples or downsamples a training set to balance classes\n",
    "# INPUT:\n",
    "# X_train = output from train_test_split function\n",
    "# y_train = output from train_test_split function\n",
    "# balance = \"upsample\", or \"downsample\"\n",
    "#          will upsample minority class or downsample majority class to balance\n",
    "#           the data\n",
    "# OUTPUT:\n",
    "# X_train = new balanced X training data\n",
    "# y_train = new balanced y training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "def class_balance(X_train, y_train, balance):\n",
    "    full_df = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "    # seperate dataframes for class\n",
    "    df_majority = full_df[full_df.outcome == 0]\n",
    "    df_minority = full_df[full_df.outcome == 1]\n",
    "\n",
    "    # upsample data if balance parameter is set to \"Upsample\" or \"upsample\"\n",
    "    if (balance == \"upsample\") | (balance == \"Upsample\"):\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                        replace=True,  # sample with replacement\n",
    "                                        n_samples=full_df[\"outcome\"].value_counts()[0.0],\n",
    "                                        # to match majority class\n",
    "                                        random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        # combine dataframes\n",
    "        full_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # downsample data if balance parameter is set to \"downsample\" or \"Downsample\"\n",
    "    elif (balance == \"downsample\") | (balance == \"Downsample\"):\n",
    "        # downsample majority class\n",
    "        # downsample majority class\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                        replace=False,  # sample without replacement\n",
    "                                        n_samples= full_df[\"outcome\"].value_counts()[1.0],\n",
    "                                        # to match minority class\n",
    "                                        random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        full_df = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    X_train = full_df.drop(columns = [\"outcome\"])\n",
    "    y_train = full_df[\"outcome\"]\n",
    "    \n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/richardlee/Desktop/JHU/2021 Fall/Precision Care Medicine/Preprocessed_Data/'\n",
    "\n",
    "\n",
    "\n",
    "#Function to get the concatenated dataframe for all positive patients\n",
    "## balance parameter can be changed to \"None\", \"upsample\", or \"downsample\"\n",
    "all_positive_patients = concat_dfs(base_path, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_patients.reset_index(drop = True, inplace=True)\n",
    "all_positive_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_patients.to_csv('newDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = all_positive_patients\n",
    "A = A.Channels.unique()\n",
    "B = []\n",
    "for channels in A:\n",
    "    electrodes = channels.split('_')\n",
    "    if electrodes[0][-1].isdigit():\n",
    "        electrodes[0] = electrodes[0][0:-1]\n",
    "    if electrodes[0][-1].isdigit():\n",
    "        electrodes[0] = electrodes[0][0:-1]\n",
    "    B.append(electrodes[0])    \n",
    "    \n",
    "    if electrodes[1][-1].isdigit():\n",
    "        electrodes[1] = electrodes[1][0:-1]\n",
    "    if electrodes[1][-1].isdigit():\n",
    "        electrodes[1] = electrodes[1][0:-1]\n",
    "    B.append(electrodes[1]) \n",
    "B = pd.Series(B)\n",
    "B.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'PY21N008'\n",
    "patient = file\n",
    "response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "df = make_df(patient, response_files_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_patients[all_positive_patients.EV==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_patients[all_positive_patients.outcome == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_patients[all_positive_patients.patient=='PY19N026']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_patients.patient.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
