{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from epilepsypcm.utils.outcome_params import seizure_onset_zone, engel_score\n",
    "\n",
    "# INPUT\n",
    "# patient = string format, patient number\n",
    "# paths = path to CCEP response files, in os format\n",
    "# OUTPUT\n",
    "# df = dataframe for one patient with\n",
    "#       X features with columns: chNames, significant, n1, n2, p2 z scores,\n",
    "#       n1, n2, p2 latencies, and flipped\n",
    "#       and associated y outcome labels\n",
    "def make_df(patient, paths):\n",
    "    #extracting info from each response file\n",
    "    n = 0\n",
    "    stimChs = []\n",
    "    for i in range(len(paths)):\n",
    "        chNames = []\n",
    "        # load info into python dictionary\n",
    "        data = json.load(open(paths[i]))\n",
    "\n",
    "        # Get list of channel names\n",
    "        for key in data[\"time\"]: chNames.append(key)\n",
    "\n",
    "        # loop over each channel, and extract average time series and information about the peaks\n",
    "\n",
    "        if n < 1:\n",
    "            avgResp = np.empty((len(paths), len(chNames), len(data['time'][chNames[0]])))\n",
    "            significant = np.empty((len(paths), len(chNames)))\n",
    "            n1Zscore = np.empty((len(paths), len(chNames)))\n",
    "            n2Zscore = np.empty((len(paths), len(chNames)))\n",
    "            p2Zscore = np.empty((len(paths), len(chNames)))\n",
    "            n1Latency = np.empty((len(paths), len(chNames)))\n",
    "            n2Latency = np.empty((len(paths), len(chNames)))\n",
    "            p2Latency = np.empty((len(paths), len(chNames)))\n",
    "            flipped = np.empty((len(paths), len(chNames)))\n",
    "            n += 1\n",
    "            samplingRate = np.empty((len(paths)))\n",
    "            window = np.empty((len(paths), 2))\n",
    "\n",
    "        for j in range(len(chNames)):\n",
    "            avgResp[i][j] = data['time'][chNames[j]]\n",
    "            significant[i][j] = data['significant'][chNames[j]]\n",
    "            n1Zscore[i][j] = data['zscores'][chNames[j]]['n1'][1]\n",
    "            n2Zscore[i][j] = data['zscores'][chNames[j]]['n2'][1]\n",
    "            p2Zscore[i][j] = data['zscores'][chNames[j]]['p2'][1]\n",
    "            n1Latency[i][j] = data['zscores'][chNames[j]]['n1'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            n2Latency[i][j] = data['zscores'][chNames[j]]['n2'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            p2Latency[i][j] = data['zscores'][chNames[j]]['p2'][0] + data['window'][0] * data[\"samplingRate\"] / 1000\n",
    "            flipped[i][j] = data['zscores'][chNames[j]]['flipped']\n",
    "\n",
    "        samplingRate[i] = data[\"samplingRate\"]\n",
    "        window[i] = data['window']\n",
    "        stimChs = stimChs + [paths[i].split(\"_\")[1] + \"_\" + paths[i].split(\"_\")[2]]*len(chNames)\n",
    "\n",
    "\n",
    "    # creating dataframe\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"stimChs\"] = stimChs\n",
    "    df[\"respChs\"] = chNames * len(paths)\n",
    "    df[\"significant\"] = significant.flatten()\n",
    "    df[\"n1Zscore\"] = n1Zscore.flatten()\n",
    "    df[\"n2Zscore\"] = n2Zscore.flatten()\n",
    "    df[\"p2Zscore\"] = p2Zscore.flatten()\n",
    "    df[\"n1Latency\"] = n1Latency.flatten()\n",
    "    df[\"n2Latency\"] = n2Latency.flatten()\n",
    "    df[\"p2Latency\"] = p2Latency.flatten()\n",
    "    df[\"flipped\"] = flipped.flatten()\n",
    "\n",
    "    # Dropped rows for stimulating channels since they only\n",
    "    # contain stimulating waveforms / artifacts / saturated signals\n",
    "    # Also zero out rows with latency values of -999.0\n",
    "\n",
    "    # drop rows in the dataframe with latency values of -999.0\n",
    "    df = df.drop(df.loc[df[\"n1Latency\"] == -999.0].index)\n",
    "    df = df.drop(df.loc[df[\"n1Latency\"] == -499.0].index)\n",
    "\n",
    "    # adding dataframe outcome values (1 if in SOZ, 0 if not)\n",
    "    df[\"outcome\"] = np.zeros(df.shape[0])\n",
    "    if engel_score[patient] == \"1\":\n",
    "        if seizure_onset_zone[patient] != \"None\":\n",
    "            for node in seizure_onset_zone[patient]:\n",
    "                for channel in df[\"respChs\"]:\n",
    "                    if node in channel:\n",
    "                        df.loc[df['respChs']==channel, ['outcome']] = 1\n",
    "    return df\n",
    "\n",
    "# Function that takes in the location of all patient folders and engel\n",
    "# score of interest, and returns a nested list of dataframes for each patient\n",
    "# INPUT:\n",
    "# base_path = string, file location to base folder that contains all patient folders\n",
    "# engel_score = string, target engel score to get dataframe for (ex. \"1\")\n",
    "#               can currently only handle \"1\" and \"2\"\n",
    "# OUTPUT:\n",
    "# positive_dataframes = a nested list, where [patient number (string), dataframe].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_processing(D):\n",
    "    D.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    #Find channel names that exists both in stimChs and respChs - only account channels that have arrows going out and in\n",
    "    overlap = []\n",
    "    for channel in D.respChs.unique():\n",
    "        if channel in D.stimChs.unique():\n",
    "            overlap.append(channel)\n",
    "\n",
    "    #Keep only the response that were stimulated in responded in the channel in overlap list\n",
    "    dropindxs = []\n",
    "    for i in range(len(D)):\n",
    "        if D.iloc[i].stimChs not in overlap or D.iloc[i].respChs not in overlap:\n",
    "                dropindxs.append(i)\n",
    "    D.drop(dropindxs,inplace=True)\n",
    "    D.reset_index(drop = True, inplace=True)\n",
    "\n",
    "    D.n1Zscore = abs(D.n1Zscore)\n",
    "    D.n2Zscore = abs(D.n2Zscore)\n",
    "    D.p2Zscore = abs(D.p2Zscore)\n",
    "    \n",
    "    #start processing\n",
    "    df = pd.DataFrame()\n",
    "    ChNames = overlap\n",
    "    Outcomes = np.array([])\n",
    "    Per_Significant_Resp = np.array([])\n",
    "    Per_Significant_Stim = np.array([])\n",
    "    N1_Avg_Resp = np.array([])\n",
    "    N1_STV_Resp = np.array([])\n",
    "    N2_Avg_Resp = np.array([])\n",
    "    N2_STV_Resp = np.array([])\n",
    "    P2_Avg_Resp = np.array([])\n",
    "    P2_STV_Resp = np.array([])\n",
    "    N1_Avg_Stim = np.array([])\n",
    "    N1_STV_Stim = np.array([])\n",
    "    N2_Avg_Stim = np.array([])\n",
    "    N2_STV_Stim = np.array([])\n",
    "    P2_Avg_Stim = np.array([])\n",
    "    P2_STV_Stim = np.array([])\n",
    "\n",
    "    for channel in ChNames:\n",
    "        Resp = D[D.respChs == channel]\n",
    "        Stim = D[D.stimChs == channel]\n",
    "\n",
    "        Outcomes = np.append(Outcomes,Resp[:1].outcome)\n",
    "\n",
    "        Per_Significant_Resp = np.append(Per_Significant_Resp,\n",
    "                                         sum(Resp.significant/len(Resp)))\n",
    "        Per_Significant_Stim = np.append(Per_Significant_Stim,\n",
    "                                         sum(Stim.significant/len(Stim)))\n",
    "\n",
    "        N1_Avg_Resp = np.append(N1_Avg_Resp,sum(Resp.n1Zscore)/len(Resp))\n",
    "        N1_STV_Resp = np.append(N1_STV_Resp,np.std(Resp.n1Zscore))\n",
    "\n",
    "        N2_Avg_Resp = np.append(N2_Avg_Resp,sum(Resp.n2Zscore)/len(Resp))\n",
    "        N2_STV_Resp = np.append(N2_STV_Resp,np.std(Resp.n2Zscore))\n",
    "\n",
    "        P2_Avg_Resp = np.append(P2_Avg_Resp,sum(Resp.p2Zscore)/len(Resp))\n",
    "        P2_STV_Resp = np.append(P2_STV_Resp,np.std(Resp.p2Zscore))\n",
    "\n",
    "        N1_Avg_Stim = np.append(N1_Avg_Stim,sum(Stim.n1Zscore)/len(Stim))\n",
    "        N1_STV_Stim = np.append(N1_STV_Stim,np.std(Stim.n1Zscore))\n",
    "\n",
    "        N2_Avg_Stim = np.append(N2_Avg_Stim,sum(Stim.n2Zscore)/len(Stim))\n",
    "        N2_STV_Stim = np.append(N2_STV_Stim,np.std(Stim.n2Zscore))\n",
    "\n",
    "        P2_Avg_Stim = np.append(P2_Avg_Stim,sum(Stim.p2Zscore)/len(Stim))\n",
    "        P2_STV_Stim = np.append(P2_STV_Stim,np.std(Stim.p2Zscore))\n",
    "\n",
    "\n",
    "\n",
    "    df['Channels'] = ChNames\n",
    "    df['outcome'] = Outcomes\n",
    "    df['SigResp'] = Per_Significant_Resp\n",
    "    df['SigStim'] = Per_Significant_Stim\n",
    "    df['N1RespAvg'] = N1_Avg_Resp\n",
    "    df['N1RespSDV'] = N1_STV_Resp\n",
    "    df['N2RespAvg'] = N2_Avg_Resp\n",
    "    df['N2RespSDV'] = N2_STV_Resp\n",
    "    df['P2RespAvg'] = P2_Avg_Resp\n",
    "    df['P2RespSDV'] = P2_STV_Resp\n",
    "    df['N1StimAvg'] = N1_Avg_Stim\n",
    "    df['N1StimSDV'] = N1_STV_Stim\n",
    "    df['N2StimAvg'] = N2_Avg_Stim\n",
    "    df['N2StimSDV'] = N2_STV_Stim\n",
    "    df['P2StimAvg'] = P2_Avg_Stim\n",
    "    df['P2StimSDV'] = P2_STV_Stim\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_df_list(base_path, engel):\n",
    "    patient_files = os.listdir(base_path)\n",
    "\n",
    "    positive_dataframes = []\n",
    "    for file in patient_files:\n",
    "        if (file[0] == \"P\") & (file != \"PY16N006\"):\n",
    "            response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "            response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "\n",
    "            # Getting individual dataframe for positive patients\n",
    "            patient = file\n",
    "            if file in engel_score.keys():  # if we currently have the file's engel score\n",
    "                if engel_score[patient] == engel:  # if the engel score is 1\n",
    "                    df = make_df(patient, response_files_path)\n",
    "                    positive_dataframes.append([patient, df])\n",
    "\n",
    "    return positive_dataframes\n",
    "\n",
    "# Function that combines dataframes for all patients of a particular\n",
    "# engel class\n",
    "# INPUT:\n",
    "# base_path = string, file location to base folder that contains all patient folders\n",
    "# engel_score = string, target engel score to get dataframe for (ex. \"1\")\n",
    "#               can currently only handle \"1\" and \"2\"\n",
    "# balance (OPTIONAL, default = None) = \"None\", \"upsample\", or \"downsample\"\n",
    "#          will upsample minority class or downsample majority class to balance\n",
    "#           the data\n",
    "# OUTPUT:\n",
    "# all_positive_patients = a concatonated dataframe of all patients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def concat_dfs(base_path, engel, balance = None):\n",
    "\n",
    "    patient_files = os.listdir(base_path)\n",
    "\n",
    "    full_df = pd.DataFrame()\n",
    "    for file in patient_files:\n",
    "        if (file[0] == \"P\") & (file != \"PY16N006\"):\n",
    "            response_path = base_path + file + '/ResponseInfo/CCEP'\n",
    "            response_files_path = glob.glob(response_path + '/*.json', recursive=True)\n",
    "\n",
    "            # Getting individual dataframe for positive patients\n",
    "            patient = file\n",
    "            if file in engel_score.keys():  # if we currently have the file's engel score\n",
    "                if engel_score[patient] == engel:  # if the engel score is 1\n",
    "                    df = make_df(patient, response_files_path)\n",
    "                    df = df_processing(df)\n",
    "                    full_df = pd.concat([full_df, df])\n",
    "                    \n",
    "        print('%s done...'%patient)\n",
    "\n",
    "    # seperate dataframes for class\n",
    "    df_majority = full_df[full_df.outcome == 0]\n",
    "    df_minority = full_df[full_df.outcome == 1]\n",
    "\n",
    "    # upsample data if balance parameter is set to \"Upsample\" or \"upsample\"\n",
    "    if (balance == \"upsample\") | (balance == \"Upsample\"):\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                         replace=True,  # sample with replacement\n",
    "                                         n_samples=full_df[\"outcome\"].value_counts()[0.0],\n",
    "                                         # to match majority class\n",
    "                                         random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        # combine dataframes\n",
    "        full_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # downsample data if balance parameter is set to \"downsample\" or \"Downsample\"\n",
    "    elif (balance == \"downsample\") | (balance == \"Downsample\"):\n",
    "        # downsample majority class\n",
    "        # downsample majority class\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                           replace=False,  # sample without replacement\n",
    "                                           n_samples= full_df[\"outcome\"].value_counts()[1.0],\n",
    "                                           # to match minority class\n",
    "                                           random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        full_df = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    return full_df\n",
    "\n",
    "\n",
    "# Function that upsamples or downsamples a training set to balance classes\n",
    "# INPUT:\n",
    "# X_train = output from train_test_split function\n",
    "# y_train = output from train_test_split function\n",
    "# balance = \"upsample\", or \"downsample\"\n",
    "#          will upsample minority class or downsample majority class to balance\n",
    "#           the data\n",
    "# OUTPUT:\n",
    "# X_train = new balanced X training data\n",
    "# y_train = new balanced y training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "def class_balance(X_train, y_train, balance):\n",
    "    full_df = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "    # seperate dataframes for class\n",
    "    df_majority = full_df[full_df.outcome == 0]\n",
    "    df_minority = full_df[full_df.outcome == 1]\n",
    "\n",
    "    # upsample data if balance parameter is set to \"Upsample\" or \"upsample\"\n",
    "    if (balance == \"upsample\") | (balance == \"Upsample\"):\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                        replace=True,  # sample with replacement\n",
    "                                        n_samples=full_df[\"outcome\"].value_counts()[0.0],\n",
    "                                        # to match majority class\n",
    "                                        random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        # combine dataframes\n",
    "        full_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # downsample data if balance parameter is set to \"downsample\" or \"Downsample\"\n",
    "    elif (balance == \"downsample\") | (balance == \"Downsample\"):\n",
    "        # downsample majority class\n",
    "        # downsample majority class\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                        replace=False,  # sample without replacement\n",
    "                                        n_samples= full_df[\"outcome\"].value_counts()[1.0],\n",
    "                                        # to match minority class\n",
    "                                        random_state=123)  # reproducible results\n",
    "\n",
    "\n",
    "        full_df = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    X_train = full_df.drop(columns = [\"outcome\"])\n",
    "    y_train = full_df[\"outcome\"]\n",
    "    \n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY21N008 done...\n",
      "PY21N006 done...\n",
      "PY20N001 done...\n",
      "PY21N007 done...\n",
      "PY17N020 done...\n",
      "PY18N007 done...\n",
      "PY17N018 done...\n",
      "PY17N018 done...\n",
      "PY16N011 done...\n",
      "PY19N009 done...\n",
      "PY17N019 done...\n",
      "PY19N012 done...\n",
      "PY19N015 done...\n",
      "PY19N023 done...\n",
      "PY19N024 done...\n",
      "PY18N015 done...\n",
      "PY18N013 done...\n",
      "PY17N005 done...\n",
      "PY21N014 done...\n",
      "PY21N022 done...\n",
      "PY20N012 done...\n",
      "PY21N012 done...\n",
      "PY20N002 done...\n",
      "PY20N005 done...\n",
      "PY21N002 done...\n",
      "PY20N003 done...\n",
      "PY21N004 done...\n",
      "PY18N003 done...\n",
      "PY19N005 done...\n",
      "PY16N013 done...\n",
      "PY17N013 done...\n",
      "PY17N014 done...\n",
      "PY18N002 done...\n",
      "PY18N002 done...\n",
      "PY18N011 done...\n",
      "PY18N016 done...\n",
      "PY19N011 done...\n",
      "PY19N018 done...\n",
      "PY19N020 done...\n",
      "PY17N008 done...\n",
      "PY17N008 done...\n",
      "PY19N026 done...\n",
      "PY16N008 done...\n",
      "PY19N017 done...\n",
      "PY21N010 done...\n",
      "PY21N020 done...\n",
      "PY20N016 done...\n",
      "PY20N011 done...\n"
     ]
    }
   ],
   "source": [
    "base_path = '/Users/richardlee/Desktop/2021 Fall/Precision Care Medicine/Coding/'\n",
    "\n",
    "\n",
    "\n",
    "#Function to get the concatenated dataframe for all positive patients\n",
    "## balance parameter can be changed to \"None\", \"upsample\", or \"downsample\"\n",
    "all_positive_patients = concat_dfs(base_path, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channels</th>\n",
       "      <th>outcome</th>\n",
       "      <th>SigResp</th>\n",
       "      <th>SigStim</th>\n",
       "      <th>N1RespAvg</th>\n",
       "      <th>N1RespSDV</th>\n",
       "      <th>N2RespAvg</th>\n",
       "      <th>N2RespSDV</th>\n",
       "      <th>P2RespAvg</th>\n",
       "      <th>P2RespSDV</th>\n",
       "      <th>N1StimAvg</th>\n",
       "      <th>N1StimSDV</th>\n",
       "      <th>N2StimAvg</th>\n",
       "      <th>N2StimSDV</th>\n",
       "      <th>P2StimAvg</th>\n",
       "      <th>P2StimSDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LA9_LA10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.530459</td>\n",
       "      <td>1.134169</td>\n",
       "      <td>2.040546</td>\n",
       "      <td>1.369091</td>\n",
       "      <td>1.304096</td>\n",
       "      <td>0.892887</td>\n",
       "      <td>7.178516</td>\n",
       "      <td>10.790559</td>\n",
       "      <td>6.536859</td>\n",
       "      <td>6.383906</td>\n",
       "      <td>5.785279</td>\n",
       "      <td>7.331337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAH1_LAH2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>7.062089</td>\n",
       "      <td>20.678040</td>\n",
       "      <td>3.320937</td>\n",
       "      <td>2.795194</td>\n",
       "      <td>2.178348</td>\n",
       "      <td>2.563587</td>\n",
       "      <td>7.883750</td>\n",
       "      <td>16.334814</td>\n",
       "      <td>4.921208</td>\n",
       "      <td>3.029861</td>\n",
       "      <td>2.352152</td>\n",
       "      <td>1.803711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAH8_LAH9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>4.032683</td>\n",
       "      <td>2.995624</td>\n",
       "      <td>3.450575</td>\n",
       "      <td>2.883998</td>\n",
       "      <td>2.376435</td>\n",
       "      <td>2.139730</td>\n",
       "      <td>2.505806</td>\n",
       "      <td>2.164878</td>\n",
       "      <td>2.190308</td>\n",
       "      <td>1.489418</td>\n",
       "      <td>2.033410</td>\n",
       "      <td>2.112725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LPH1_LPH2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.522655</td>\n",
       "      <td>17.219698</td>\n",
       "      <td>3.369771</td>\n",
       "      <td>3.506524</td>\n",
       "      <td>2.281413</td>\n",
       "      <td>3.303814</td>\n",
       "      <td>6.505212</td>\n",
       "      <td>8.264811</td>\n",
       "      <td>4.630948</td>\n",
       "      <td>3.574869</td>\n",
       "      <td>4.187831</td>\n",
       "      <td>3.626223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LPH7_LPH8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>7.304854</td>\n",
       "      <td>10.728727</td>\n",
       "      <td>4.610356</td>\n",
       "      <td>4.159990</td>\n",
       "      <td>3.468193</td>\n",
       "      <td>2.645476</td>\n",
       "      <td>2.823084</td>\n",
       "      <td>2.925869</td>\n",
       "      <td>1.606341</td>\n",
       "      <td>1.075715</td>\n",
       "      <td>1.461462</td>\n",
       "      <td>1.012732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RMBT2_RMBT3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>10.029460</td>\n",
       "      <td>19.122649</td>\n",
       "      <td>6.391055</td>\n",
       "      <td>5.733855</td>\n",
       "      <td>5.789198</td>\n",
       "      <td>8.766071</td>\n",
       "      <td>13.594587</td>\n",
       "      <td>19.912187</td>\n",
       "      <td>6.165966</td>\n",
       "      <td>5.398174</td>\n",
       "      <td>7.684954</td>\n",
       "      <td>9.885282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RPTI19_RPTI20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>13.323164</td>\n",
       "      <td>20.134989</td>\n",
       "      <td>4.393888</td>\n",
       "      <td>4.366079</td>\n",
       "      <td>7.924898</td>\n",
       "      <td>11.404889</td>\n",
       "      <td>14.164498</td>\n",
       "      <td>21.710392</td>\n",
       "      <td>6.080624</td>\n",
       "      <td>5.949018</td>\n",
       "      <td>6.706642</td>\n",
       "      <td>9.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RPTS60_RPTS61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>9.883556</td>\n",
       "      <td>20.481622</td>\n",
       "      <td>4.603904</td>\n",
       "      <td>6.318618</td>\n",
       "      <td>5.756485</td>\n",
       "      <td>9.535264</td>\n",
       "      <td>9.726907</td>\n",
       "      <td>15.484303</td>\n",
       "      <td>6.323287</td>\n",
       "      <td>5.584253</td>\n",
       "      <td>7.569330</td>\n",
       "      <td>11.384782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RPFS4_RPFS5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.462636</td>\n",
       "      <td>1.320800</td>\n",
       "      <td>3.635232</td>\n",
       "      <td>2.139720</td>\n",
       "      <td>1.444058</td>\n",
       "      <td>0.870770</td>\n",
       "      <td>1.916434</td>\n",
       "      <td>1.392987</td>\n",
       "      <td>3.160595</td>\n",
       "      <td>1.173929</td>\n",
       "      <td>3.855516</td>\n",
       "      <td>3.516656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RPTI26_RPTI27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>14.681348</td>\n",
       "      <td>23.509883</td>\n",
       "      <td>6.121649</td>\n",
       "      <td>7.522032</td>\n",
       "      <td>7.386405</td>\n",
       "      <td>12.493640</td>\n",
       "      <td>8.645658</td>\n",
       "      <td>14.446502</td>\n",
       "      <td>3.828483</td>\n",
       "      <td>1.756581</td>\n",
       "      <td>4.477822</td>\n",
       "      <td>5.954399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Channels  outcome   SigResp   SigStim  N1RespAvg  N1RespSDV  \\\n",
       "0        LA9_LA10      1.0  0.000000  0.250000   1.530459   1.134169   \n",
       "1       LAH1_LAH2      1.0  0.076923  0.250000   7.062089  20.678040   \n",
       "2       LAH8_LAH9      0.0  0.307692  0.125000   4.032683   2.995624   \n",
       "3       LPH1_LPH2      0.0  0.115385  0.333333   6.522655  17.219698   \n",
       "4       LPH7_LPH8      0.0  0.307692  0.083333   7.304854  10.728727   \n",
       "..            ...      ...       ...       ...        ...        ...   \n",
       "4     RMBT2_RMBT3      0.0  0.250000  0.375000  10.029460  19.122649   \n",
       "5   RPTI19_RPTI20      0.0  0.250000  0.250000  13.323164  20.134989   \n",
       "6   RPTS60_RPTS61      0.0  0.125000  0.250000   9.883556  20.481622   \n",
       "7     RPFS4_RPFS5      0.0  0.000000  0.000000   2.462636   1.320800   \n",
       "8   RPTI26_RPTI27      0.0  0.250000  0.250000  14.681348  23.509883   \n",
       "\n",
       "    N2RespAvg  N2RespSDV  P2RespAvg  P2RespSDV  N1StimAvg  N1StimSDV  \\\n",
       "0    2.040546   1.369091   1.304096   0.892887   7.178516  10.790559   \n",
       "1    3.320937   2.795194   2.178348   2.563587   7.883750  16.334814   \n",
       "2    3.450575   2.883998   2.376435   2.139730   2.505806   2.164878   \n",
       "3    3.369771   3.506524   2.281413   3.303814   6.505212   8.264811   \n",
       "4    4.610356   4.159990   3.468193   2.645476   2.823084   2.925869   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "4    6.391055   5.733855   5.789198   8.766071  13.594587  19.912187   \n",
       "5    4.393888   4.366079   7.924898  11.404889  14.164498  21.710392   \n",
       "6    4.603904   6.318618   5.756485   9.535264   9.726907  15.484303   \n",
       "7    3.635232   2.139720   1.444058   0.870770   1.916434   1.392987   \n",
       "8    6.121649   7.522032   7.386405  12.493640   8.645658  14.446502   \n",
       "\n",
       "    N2StimAvg  N2StimSDV  P2StimAvg  P2StimSDV  \n",
       "0    6.536859   6.383906   5.785279   7.331337  \n",
       "1    4.921208   3.029861   2.352152   1.803711  \n",
       "2    2.190308   1.489418   2.033410   2.112725  \n",
       "3    4.630948   3.574869   4.187831   3.626223  \n",
       "4    1.606341   1.075715   1.461462   1.012732  \n",
       "..        ...        ...        ...        ...  \n",
       "4    6.165966   5.398174   7.684954   9.885282  \n",
       "5    6.080624   5.949018   6.706642   9.411200  \n",
       "6    6.323287   5.584253   7.569330  11.384782  \n",
       "7    3.160595   1.173929   3.855516   3.516656  \n",
       "8    3.828483   1.756581   4.477822   5.954399  \n",
       "\n",
       "[309 rows x 16 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_patients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
